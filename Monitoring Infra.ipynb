{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoring infra setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ `Prerequisites` ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Within VDI ]\n",
    "\n",
    "> download <mark>Miniconda3-py37_4.8.2-Linux-x86_64.sh</mark> from https://repo.anaconda.com/miniconda/\n",
    "\n",
    "> download & install WinSCP\n",
    "\n",
    "> upload Miniconda3-py37_4.8.2-Linux-x86_64.sh to <mark>Telegraf host</mark> to /tmp or /home/${USER}/UI/install_files (but first create the folder for that - see below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Telegraf host CLI ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`update bashrc for the monitoring user:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run once:\n",
    "\n",
    "cleantext=\"\n",
    "export HISTTIMEFORMAT=\"[%Y-%m-%d %H:%M:%S] \"\n",
    "HISTSIZE='INFINITY'; HISTFILESIZE='ANDBEYOND'\n",
    "\n",
    "PS1='\\e[37m\\D{%H:%M}\\e[91m[\\e[90m\\u@\\h \\e[33m\\w\\e[31m]\\e[92m\\n\\$'\n",
    "\n",
    "alias ll='ls -alF'\n",
    "alias la='ls -A'\n",
    "alias l='ls -CmF'\n",
    "alias lr='ls -ltrh'\n",
    "alias ufind=\"find / -name $1 2>/dev/null\"\n",
    "\n",
    "export PATH=$PATH:/home/${USER}/scripts\n",
    "\"\n",
    "\n",
    "echo \"$cleantext\" >> /home/${USER}/.bashrc.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check OS vesion:\n",
    "cat /etc/os-release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(below commands are for RHEL distro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install prereqs with elevated (sudo) user:\n",
    "\n",
    "# prod:\n",
    "yum install -y alsa-lib bc gcc gcc-c++ kernel-devel libXScrnSaver libXcomposite libXcursor libXdamage libXi libXrandr libXtst libffi-devel libxslt-devel mesa-libEGL mesa-libGL msodbcsql18.x86_64 openssl-devel unixODBC-devel\n",
    "# dev:\n",
    "subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms # required for x11\n",
    "yum install -y alsa-lib bc gcc gcc-c++ kernel-devel libXScrnSaver libXcomposite libXcursor libXdamage libXi libXrandr libXtst libffi-devel libxslt-devel mesa-libEGL mesa-libGL msodbcsql18.x86_64 openssl-devel unixODBC-devel xorg-x11-apps xorg-x11-xauth firefox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders:\n",
    "mkdir -p  /home/${USER}/UI/Flask ~/UI/install_files\n",
    "\n",
    "# give execute rights to the installed:\n",
    "chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
    "#     Long press Enter; then \"yes\"; then \"yes\" (again)\n",
    "#     when completed - reload shell:\n",
    ". ~/.bashrc\n",
    "\n",
    "# install requirements for Flask API (maybe you need to update the path to requirements folder: check \"flask_wapi_UAT\")\n",
    "cd  ~/UI/flask_wapi_UAT/requirements; `for': for file in $(ls) ; do pip install ./${file}; done\n",
    "#    check if all are required (wheel; tar.gz.. some might be duplicates)\n",
    "#    install the remaining (not included - since I've changed from Anaconda to miniconda) packages via proxy command:\n",
    "for package in Flask python-dotenv pandas pyodbc; do pip install $package --proxy \"http://USER:XXXX@XXX.XXX.XXX.XX:XXXX\"; done\n",
    "\n",
    "# open the firewall port:\n",
    "firewall-cmd --add-port=8000/tcp\n",
    "firewall-cmd --add-port=8000/tcp --permanent\n",
    "\n",
    "# load the flask config and start the UI:\n",
    ". ../.flaskenv\n",
    ". ../.flask run\n",
    "#    [dev/test instance]\n",
    "flask run --host 0.0.0.0  \n",
    "#    [Prod]\n",
    "IP=\"$(hostname -I | awk '{print $1}')\"\n",
    "nohup flask run --host $IP &\n",
    "# to close the Flask UI type \"fg\" and press ctrl+c; or \"kill %1\" -but be sure that is the only background process that is running! or 'pkill flask'; to close all related tasks: 'sudo killall -u ${USER}'  # note that they will still be running from cron or when the server is restarted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VDI] `UI`\n",
    "> open any preferred web browser (i.e.: Brave/Edge/Firefox/Chrome/Opera...)\n",
    "\n",
    "> open the UI: http://<mark>XX.X.XX.XX</mark>:8000 #replace with correct IP address <mark># note that the firewall port 8000 has to be opened!</mark> (as stated above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Telegraf host] `cli backend scripts and scheduled cronjobs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CHANGELOG // current version = v1.09; 2022.10.19 (Author: Michal MÃ¡rkus)\n",
    " for changes prior to 9.02 please check the meeting invite \"Infra Self Monitoring (agenda)\"\n",
    "- 9.02: Added harvest check \n",
    "- 09.06: Added uptime reporting (not included in this script; new script is called \"instert_uptime.sh\" & is cronned to run daily 5 minutes after midnight)\n",
    "- 9.13: solution migrated to PROD & configuration adjusted; added influx-bucet monitoring\n",
    "- 10.04: assets (grafana/influx/harvest/telegraf) added to Radix Shared cockpit (needed for maintenance & uptime reporting)\n",
    "- 10.11: logrotation for the backend script logs added (for more details look for '/etc/logrotate.d/flask' below); NodeRed is now also monitored; ticket creation validated on PROD - is working; maintenance info added to the UI as well as 'past incidents' tab got enhanced from both frontend & backend perspective; added daily backup cronjob which backs up the most crucial parts of the monitoring solution (flask, scripts, configuration files); pushed the current version to GitLab\n",
    "- 10.19: added failover solution to ticketing; fixed many minor things (such as: silenced curl verbose standard output for some services as for when the script is called manually - instead added progressbar to show what the script is doing currenlty and print how long it run; fixed havest ticket creation #as more detailed tracing broke the proper aquisition of hostname - similarly at serviceup5min function...); moved the ENV specific variables to the beginning of the script so that it can be more easily deployed/changed/migrated; added some comments and refined the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ `Backend` ]\n",
    "### <mark>monitoring_services.sh</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create backend script:\n",
    "vi /home/${USER}/monitoring_services.sh  \n",
    "# ^this is the script that is cronned; the other one '/home/${USER}/monitoring_services' (note that there is no \".sh\" extension) serves as 'pre-prod' script to manually check and debug when new features are added or something is to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash \n",
    "\n",
    "# TIMER -start\n",
    "res1=$(date +%s.%N)\n",
    "# measure runtime of this script\n",
    "\n",
    "\n",
    "# D E B U G  M O D E\n",
    "# set -x\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "#       \"\"\" CHANGE THESE VALUES ACCORDING TO YOUR ENVIRONMENT:\"\"\"\n",
    "\n",
    "flask_path=/home/${USER}/UI/flask_wapi\n",
    "ENVIRONMENT=\"EU PROD\"  # change depending on environment (can be: EU PROD / US / APAC)\n",
    "services=\"grafana harvest influx nodered telegraf\"\n",
    "grafana_url=\"https://grafana.apps.XXXX.xxxgroup.net/api/health\" \n",
    "HARVEST_HOSTS=\"HarvestHostXXX1 HarvestHostXXX2 HarvestHostXXX3...\"\n",
    "influx_url=\"https://influxdb.apps.XXXX.xxxgroup.net/health\"\n",
    "nodered_endpoint_url=\"https://nodered.apps.XXXX.xxxgroup.net/admin/\"\n",
    "primary_endpoint=https://XXXX.xxxgroup.net:8443/bppmws/api/Event/create?routingId=pforwemcellXX\n",
    "secondary_endpoint=https://XXXX.xxxgroup.net:8443/bppmws/api/Event/create?routingId=pforwemcellXX\n",
    "\n",
    "#          NOTE: In the notebook I've omitted the password for ${USER} user due to compliance&security reasons\n",
    "                          # If copying the script from here (instead GitLab); then\n",
    "                          #    update the \"USER_PASSWORD\" with proper values!!!\n",
    "                          #    otherwise harvest service will not be monitored\n",
    "        \n",
    "#           READ THIS! - YOU HAVE TO also MANUALLY UPDATE the following items:\n",
    "                #event_id (under create_ticket part)\n",
    "\n",
    "                           # EU PROD IDs start with XXX10\n",
    "                           # EU UAT IDs start with XXX00\n",
    "                           # US IDs start with XXX20\n",
    "                           # Apac IDs start with XXX30\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "\n",
    "\n",
    "DATE=`date +'%m/%d/%Y %H:%M:%S'`\n",
    "err_msg=\"not running @$DATE\"\n",
    "ok_msg=\"OK @$DATE\"\n",
    "cd $flask_path\n",
    "\n",
    "\n",
    "\n",
    "# Define function to check service status:\n",
    "services_check()\n",
    "{\n",
    "        echo -e '\\e[1A\\e[K\\nchecking service status for'\n",
    "        echo -ne '          (0%)\\r'\n",
    "\n",
    "\n",
    "# ACTIVE_IQ\n",
    "    # not defined yet\n",
    "\n",
    "# GRAFANA:\n",
    "    grafana_status()\n",
    "    {\n",
    "\techo -e '\\e[1A\\e[Kchecking service status for Grafana'\n",
    "\techo -ne '#                   (5%)\\r'\n",
    "\n",
    "        grafana_url=$grafana_url # PROD (port is 443)\n",
    "        grafana_check=\"$(curl -s $grafana_url | grep -oh [[:alpha:]]*ok[[:alpha:]]*)\"  # checks if status is \"ok\"\n",
    "        grafana_latency=$(curl -s -w 'Establish Connection: %{time_connect}s\\nTTFB: %{time_starttransfer}s\\nTotal: %{time_total}s\\n' 127.0.0.1:3000/ping/api/health | egrep \"Total: [1-9]\"|cut -d' ' -f2) ;  # checks if latency is above 1 second\n",
    "        # log status (possible error: since the nested if there might be a case when the url is not valid, however only latency will be reported...):\n",
    "        [ $(eval echo \\$\"${service}_check\") == 'ok' ] && [ -z \"$latency\" ] ||  echo -e \"$DATE $service latency is ${service}_latency high\" >> ${service}_high_latency.log && echo \"${service} $ok_msg\" >> ${service}_uptime.log || echo \"${service}\" $err_msg >> ${service}_uptime.log\n",
    "        export grafana_latency\n",
    "    }\n",
    "\n",
    "# HARVEST:\n",
    "    harvest_status()\n",
    "    {\n",
    "\techo -e '\\e[1A\\e[Kchecking service status for Harvest'\n",
    "\techo -ne '##                  (10%)\\r'\t\n",
    "\n",
    "        for H in $HARVEST_HOSTS;\n",
    "            do\n",
    "                harvest_status=\"$(echo \"QQ-USER_PASSWORD\"  | /home/${USER}/scripts/.hrp ssh ${USER}@${H} 'systemctl status harvest')\"\n",
    "                echo \"$harvest_status\" > harvest_status_${H}.txt\n",
    "                harvest_check=$(echo \"$harvest_status\" | sort -u | grep running | wc -l)\n",
    "                if ! [ \"$(echo $harvest_check)\" == 1 ]; then echo \"${service}\" $err_msg >> ${service}_${H}_uptime.log && echo \"${service}| $err_msg |$H\" >> harvest_uptime.log; else echo \"${service} $ok_msg\" >> ${service}_${H}_uptime.log; fi\n",
    "            done\n",
    "        harvest_status=\"$(cat harvest_status*.txt)\"; export harvest_status\n",
    "        pollers=\"$(echo $harvest_status| grep \"not running\")\"\n",
    "    }\n",
    "\n",
    "# INFLUX:\n",
    "    influx_status()\n",
    "    {\n",
    "\techo -e '\\e[1A\\e[Kchecking service status for Influx'\t\n",
    "\techo -ne '###                 (15%)\\r'\n",
    "\n",
    "        influx_url=$influx_url  # PROD (port is 443, but that doesn't have to be explicitly defined as it is the default gateway)\n",
    "        influx_check=\"$(curl -s $influx_url | grep status |  grep -oh [[:alpha:]]*pass[[:alpha:]]*)\"  # check if status is \"pass\"\n",
    "        influx_latency=$(curl -s -w 'Establish Connection: %{time_connect}s\\nTTFB: %{time_starttransfer}s\\nTotal: %{time_total}s\\n' 127.0.0.1:8086 | egrep \"Total: [0-9]\"|cut -d' ' -f2)\n",
    "        [ $(eval echo \\$\"${service}_check\") == 'pass' ] && [ -z \"$latency\" ] || echo -e \"$service latency is $influx_latency high @$DATE\" | tee -a ${service}_high_latency.log ${service}_uptime.log && echo \"${service} $ok_msg\" >> ${service}_uptime.log || echo \"${service}\" $err_msg  >> ${service}_uptime.log && influx_latency_err=\"$influx_latency\"\n",
    "\n",
    "        influx_bucket_write_check=\"$(cat /etc/telegraf/logs/*.log | grep -i \"error writing\" | egrep \"`date +'%Y-%m'`\" | egrep `date +'%H:%M:'` | wc -l)\"\n",
    "        if [ \"$(echo $influx_bucket_write_check)\" -gt 1 ]; then influx_bucket_write_status=\"Write ERROR\"; echo -e \"$service cannot write into buckets @$DATE\" | tee -a  ${service}_write_err.log ${service}_uptime.log; fi\n",
    "        export influx_latency_err\n",
    "        export influx_bucket_write_status\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "# NodeRed\n",
    "    nodered_status()\n",
    "    {\n",
    "        echo -e '\\e[1A\\e[Kchecking service status for NodeRed'\n",
    "        echo -ne '####                (20%)\\r'\n",
    "\n",
    "\tnodered_url=$nodered_endpoint_url\n",
    "        nodered_check=\"$(curl -s -o - -I \"$nodered_url\" -X GET | grep -oh [[:alpha:]]*OK[[:alpha:]]*)\"\n",
    "        [ $(eval echo \\$\"${service}_check\") == 'OK' ] && echo \"${service} $ok_msg\" >> ${service}_uptime.log || echo \"${service}\" $err_msg >> ${service}_uptime.log\n",
    "        export nodered_check\n",
    "    }\n",
    "\n",
    "\n",
    "# TELEGRAF:\n",
    "    telegraf_status()\n",
    "    {\n",
    "\techo -e '\\e[1A\\e[Kchecking service status for Telegraf'\n",
    "\techo -ne '####                (20%)\\r'\n",
    "\n",
    "\tsystemctl | grep telegraf | sort -u | grep running > telegraf_status.txt  # needed for flask UI\n",
    "\n",
    "        telegraf_check=\"$(systemctl | grep telegraf | sort -u | grep running | grep not | awk '{print $1}')\"\n",
    "        telegraf_check_count=$(echo \"$telegraf_check\"  | wc -l)\n",
    "        if [ -z \"$telegraf_check_count\" ]; then echo \"${service}\" $err_msg >> ${service}_uptime.log; else echo \"${service} $ok_msg\" >> ${service}_uptime.log; fi\n",
    "        export telegraf_check\n",
    "    }\n",
    "\n",
    "# LOOP OVER SERVICES to check status:\n",
    "for service in $services \n",
    "    do\n",
    "        ${service}_status\n",
    "    done\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# send ticket (directly to Remedy) if service is down for 5 consecutive minutes (checked by every minute via cronjob)\n",
    "\n",
    "ticket()\n",
    "{\n",
    "\n",
    "EPOCHNOW=`date -d \"${DATE}\" +\"%s\"`\n",
    "\n",
    "    serviceup5min()\n",
    "        {\n",
    "        echo -e '\\e[1A\\e[Kchecking if any service was down for the past 5 consecutive minutes'\n",
    "        echo -ne '######              (30%)\\r'\n",
    "\n",
    "        c=0  # counts how many times the app was down ruing the past 5 minutes (0 = no outage; 1..4 partial; 5 = app is down for 5 minutes)\n",
    "        for((i=1;i<=5;++i))\n",
    "            do\n",
    "            stat=$(tac ${service}_*uptime.log | sed -n \"${i},1p\")\n",
    "            dat=$(echo $stat| cut -d@ -f2| cut -d'|' -f1)\n",
    "\n",
    "            if [[ \"$stat\" == *\"OK\"*  ]]; then return 1\n",
    "\n",
    "            else\n",
    "                epoch_dat=`date -d \"${dat}\" +\"%s\"`\n",
    "                if [ \"$(echo $EPOCHNOW-$epoch_dat|bc)\" -le \"360\"  ] # less or equal to 360 seconds AKA 6 min (5min +1min grace time due to latency)\n",
    "                    then c=$((c+1))\n",
    "                    export c\n",
    "                    if [ \"$c\" == 1 ]; then echo \"donwtime `date  +\"%m/%d %H:%M:%S\"` /\" >>  ${service}_downtime.rep; fi\n",
    "                fi\n",
    "            fi\n",
    "\n",
    "            done\n",
    "        }\n",
    "\n",
    "\n",
    "        create_event()  # only create & send event if it is down for 5 consecutive minutes - this part is skipped otherwise\n",
    "                {\n",
    "\t\n",
    "\t\techo -e '\\e[1A\\e[Kcreating ticket'\n",
    "        \techo -ne '#######             (35%)\\r'\n",
    "\n",
    "                        if [[ $c -ne 5 ]]; then return 1  # debug mode is: \"-eq\"; normal mode is: \"-ne\" (not equal)\n",
    "                        else\n",
    "                                # ADAPTER_HOST  ## Name of the host where the evet was created == Telegraf (always)\n",
    "                                adapter_host=`hostname`\n",
    "\n",
    "                                # MSG is dynamically generated message; consists of the parts below:\n",
    "                                if [[ \"$service\" == \"telegraf\" ]]; then resolver_team=\"Monitoring\";\n",
    "\t\t\t\telif [[ \"$service\" == \"harvest\" ]]; then resolver_team=\"Storage\"; \n",
    "\t\t\t\telse resolver_team=\"Linux\"; fi\n",
    "\n",
    "                                # HOSTNAME / DD (unique differentioator - description) / EventID -- UPDATE ID ACCORDING YOUR ENVIRONMENT!\n",
    "                                        # EU PROD IDs start with XXX10\n",
    "                                        # EU UAT IDs start with XXX00\n",
    "                                        # US IDs start with XXX20\n",
    "                                        # Apac IDs start with XXX30\n",
    "                                # DD for telegraf is sub processes; for harvest pollers; for grafana latency; for influx latency & write issue\n",
    "                                if [[ \"$service\" == \"telegraf\" ]]; then hname=`hostname` && dd=$telegraf_check && event_id=\"XXX10\"\n",
    "                                elif [[ \"$service\" == \"grafana\" ]]; then hname=\"localhost\" && dd=\"$grafana_latency\" && event_id=\"XXX11\"\n",
    "                                elif [[ \"$service\" == \"influx\" ]]; then hname=\"localhost\" && dd=\"echo $influx_latency_err $influx_bucket_write_status\" && event_id=\"XXX12\"\n",
    "                                elif [[ \"$service\" == \"harvest\" ]]; then H=$(find $flask_path -name 'harvest*.log' -exec grep 'not running' {} \\; -print | grep log | sort -u | cut -d_ -f4 | grep -v uptime); hname=$H && dd=$pollers && event_id=\"XXX13\"\n",
    "                                elif [[ \"$service\" == \"nodered\" ]]; then hname=`hostname` && dd=$nodered_check && event_id=\"XXX14\"\n",
    "                                fi\n",
    "\n",
    "                                # msg Example: \"[$resolver_team] CRITICAL: $service is down $date, $details\"\n",
    "                                msg=\"Ã$resolver_teamÂ¤ CRITICAL: $service is down,DATE\"\n",
    "\n",
    "                                # CONTRACT_ID\n",
    "                                ENV=$ENVIRONMENT\n",
    "\n",
    "                                if [[ \"$ENV\" == \"EU PROD\" ]]; then contract_id=\"XXXXXXXXXXXXX\"\n",
    "                                elif [[ \"$ENV\" == \"EU UAT\" ]]; then contract_id=\"XXXXXXXXXXXXX\"\n",
    "                                elif [[ \"$ENV\" == \"US\" ]]; then contract_id=\"XXXXXXXXXXXXX\"\n",
    "                                elif [[ \"$ENV\" == \"APAC\" ]]; then contract_id=\"XXXXXXXXXXXXX\"\n",
    "                                fi\n",
    "\n",
    "\n",
    "                   # SED process - create event.json from json.SED file\n",
    "                                cd /home/${USER}/scripts/ticket\n",
    "                                cat json.SED > event.json  # json.SED is a template with high caps pseudo 'variables' which are replaced with small caps real variables\n",
    "                                for REPLACE in ADAPTER_HOST MSG CONTRACT_ID HNAME EVENT_ID SERVICE DD; do replace=${REPLACE,,}; sed -i \"s/${REPLACE}/${!replace}/g\" event.json; done\n",
    "                                sed -i \"s/Ã/\\[/g\" event.json; sed -i \"s/Â¤/\\]/g\" event.json  # this is to replace the brackets around the $resolver_team as otherwise it would mess up the sed command...\n",
    "                                sed -i \"s/DATE/$(date +'%Y\\/%m\\/%d %H:%M:%S')/g\" event.json  # this replaces the date in DD2 field; again this has to be explicitly this way, because otherwise sed fails to do what it should.\n",
    "\n",
    "                                cd -\n",
    "                        fi\n",
    "                }\n",
    "\n",
    "\n",
    "    send_event()\n",
    "        {\n",
    "\techo -e '\\e[1A\\e[Kprocessing ticket'\n",
    "        echo -ne '#######             (35%)\\r'\n",
    "\n",
    "\tif [[ $c -eq 5 ]]; then\n",
    "\n",
    "        cd /home/${USER}/scripts/ticket\n",
    "\n",
    "        primary_endpoint=$primary_endpoint\n",
    "        if ! [ -z $secondary_endpoint ]; then secondary_endpoint=$secondary_endpoint; else secondary_endpoint=$primary_endpoint; fi\n",
    "        curl_cmd='curl -o - -s  -k -d \"@event.json\" -H \"Content-Type: application/json\" -H \"authorization: basic cXFreTAyMDpUNkhLeWdfUjVrcmQ0M0s= \" -X POST '\n",
    "\n",
    "        eval $(echo \"$curl_cmd $primary_endpoint\") | grep 'statusCode\":\"200\"' > ep1.out\n",
    "        if ! [[ \"$(cat ep1.out)\"  == *'statusCode\":\"200\"'* ]]; then\n",
    "          echo -e \"\\e[1A\\e[Ksending ticket to primary endpoint failed - sending to secondary\";\n",
    "          echo \"Endpoint: $primary_endpoint, status: $(cat ep1.out)\" >> ticket_send_error.log;\n",
    "          eval $(echo \"$curl_cmd $secondary_endpoint\") | grep 'statusCode\":\"200\"' > ep2.out\n",
    "          if [[ \"$(cat ep2.out)\"  == *'statusCode\":\"200\"'* ]]; then\n",
    "            echo -e \"\\e[1A\\e[KTicket succesfully sent\"; else\n",
    "            echo -e \"\\e[1A\\e[Ksending ticket FAILED\";\n",
    "            # if ticket was not sent to any of the endpoints then at least write it to a logfile:\n",
    "            echo \"Endpoint: $primary_endpoint, status: $(cat ep2.out)\" >> ticket_send_error.log;\n",
    "          fi\n",
    "        fi\n",
    "\n",
    "        cd -\n",
    "\n",
    "\tfi\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "    for service in $services\n",
    "        do\n",
    "\t\tserviceup5min && create_event && send_event  # as mentioned above the create event and sendevent only runs if a service is down for 5 conseq. miniutes\n",
    "        done\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "manage_logs()\n",
    "    {\n",
    "\n",
    "\techo -e '\\e[1A\\e[Kgenerating log files'\n",
    "        echo -ne '########            (40%)\\r'\n",
    "\n",
    "        # GENERATE UPTIME LOGS FOR FLASK\n",
    "        service_uptime()  # Grafana & Influx\n",
    "        {\n",
    "\techo -e '\\e[1A\\e[Kgenerating log files - for telegraf'\n",
    "        echo -ne '########            (40%)\\r'\t\n",
    "\n",
    "                if [[ \"$service\" = \"telegraf\" ]]; then\n",
    "                telegraf_sub_service_upt()\n",
    "                    {\n",
    "                    stat=`systemctl status telegraf_${sub}.service`\n",
    "                    [[ $(echo \"$stat\" | grep \"running\") == *running* ]] && r=\"Running\" || r=\"Down\"\n",
    "                    REPORT=$(echo $stat |tr ' ' '\\n' |grep -A4 since | tail -3 | tr '\\n' ' '|cut -d';' -f1)\n",
    "\n",
    "                    echo -e \"$r|since $REPORT\" > telegraf_${sub}_uptime.txt\n",
    "\n",
    "                    }\n",
    "\n",
    "                for sub in esx system traps  #broadcom cisco esx storage system traps (these were defined only on UAT)\n",
    "                do\n",
    "                    telegraf_sub_service_upt\n",
    "                done\n",
    "\n",
    "\n",
    "                elif [[ \"$service\" = \"harvest\" ]]; then\n",
    "                harvest_services_upt()\n",
    "                    {\n",
    "\t\t    echo -e '\\e[1A\\e[Kgenerating log files for harvest'\n",
    "\t\t    echo -ne '##########          (50%)\\r'\n",
    "\n",
    "                    stat=\"$harvest_status|grep $H\"\n",
    "                    [[ $(echo \"$stat\" | grep \"running\") == *running* ]] && r=\"Running\" || r=\"Down\"\n",
    "                    REPORT=$(echo $stat |tr ' ' '\\n' |grep -A4 since | tail -3 | tr '\\n' ' '|cut -d';' -f1)\n",
    "\n",
    "                    echo -e \"$r|since $REPORT\" > harvest_${H}_uptime.txt\n",
    "\n",
    "                    }\n",
    "\n",
    "                for H in $HARVEST_HOSTS \n",
    "                do\n",
    "                    harvest_services_upt\n",
    "                done\n",
    "\n",
    "\n",
    "                else\n",
    "\t\techo -e '\\e[1A\\e[Kgenerating log files for grafana & influx'\n",
    "        \techo -ne '###############     (75%)\\r'\n",
    "                \n",
    "\t\tlast=$(tac ${service}_uptime.log | grep -A1 -m 1 \"not\")  # sample: grafana OK @08/11/2022 17:28:03\n",
    "                up=$(echo \"$last\" | tail -1)\n",
    "                epoch_up=`date -d \"$(echo $up | cut -d@ -f2)\" +\"%s\"`\n",
    "                down=$(echo \"$last\" | head -1)\n",
    "                epoch_down=`date -d \"$(echo $down | cut -d@ -f2)\" +\"%s\"`\n",
    "                prev_down=$(tac ${service}_uptime.log | grep -m 2 \"not\" | tail -1)\n",
    "                epoch_prev_down=`date -d \"$(echo $prev_down | cut -d@ -f2)\" +\"%s\"`\n",
    "                seconds=`echo \"$epoch_down\"-\"$epoch_prev_down\"|bc`\n",
    "\n",
    "                if [[ \"$seconds\" -eq 0 ]]\n",
    "                        then echo \"UNKNOWN\" > ${service}_uptime.txt\n",
    "                else\n",
    "                        downtime_minutes=$(echo $seconds/60|bc)\n",
    "                        service_uptime=`echo $(echo \"$EPOCHNOW\"-\"$epoch_up\"|bc)/60|bc`\n",
    "\n",
    "                        REPORT=$(date --date=\"$service_uptime\" +\"%m/%d %H:%M:%S\")\n",
    "\n",
    "                        echo -e \"Down: $down|Up:$up\\nOutage Time: $last_outage minutes|$REPORT\" > ${service}_uptime.txt\n",
    "                fi\n",
    "            fi\n",
    "        }\n",
    "\n",
    "       for service in $services \n",
    "        do\n",
    "          service_uptime\n",
    "        done\n",
    "\n",
    "\n",
    "        past_incidents()\n",
    "        {\n",
    "\n",
    "\t# PAST INCIDENTS part has been outsourced to ANOHTER SCTIPT (past_incidents.sh) that is scheduled hourly\n",
    "\t# this part only ensures that the UI always has these files, else the dinamically generated webpage breaks\n",
    "\n",
    "\tfor f in montly weekly today;\n",
    "\t    do \n",
    " \t        touch ${f}.csv\n",
    "\t    done   \n",
    "\n",
    "\t}\n",
    "\n",
    "\tpast_incidents\n",
    "\n",
    "        echo -ne '####################(100%)\\r'\n",
    "        echo -ne '\\n'\n",
    "\n",
    "\techo -e '\\e[1A\\e[K '\n",
    "\techo -e '\\e[2A\\e[K...completed!'\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Run main parts of the script:\n",
    "\n",
    "services_check\n",
    "ticket\n",
    "manage_logs\n",
    "\n",
    "\n",
    "\n",
    "# TIMER STOP (calculate runtime):\n",
    "res2=$(date +%s.%N)\n",
    "dt=$(echo \"$res2 - $res1\" | bc)\n",
    "dd=$(echo \"$dt/86400\" | bc)\n",
    "dt2=$(echo \"$dt-86400*$dd\" | bc)\n",
    "dh=$(echo \"$dt2/3600\" | bc)\n",
    "dt3=$(echo \"$dt2-3600*$dh\" | bc)\n",
    "dm=$(echo \"$dt3/60\" | bc)\n",
    "ds=$(echo \"$dt3-60*$dm\" | bc)\n",
    "echo\n",
    "printf \"script run for: %d:%02d:%02d:%02.4f\\n\" $dd $dh $dm $ds\n",
    "echo\n",
    "set +x\n",
    "\n",
    "#exit 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to run `systemctl status harvest` <b>remotely</b> you need to have the script below:\n",
    "> `vi ~/home/${USER}/.hrp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash \n",
    "[[ $1 =~ password: ]] && cat || SSH_ASKPASS=\"$0\" DISPLAY=nothing:0 exec setsid \"$@\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or simply [create passwordless ssh](https://linuxize.com/post/how-to-setup-passwordless-ssh-login/) between the hosts (for me this option was not permitted); \n",
    "- in that case you'll need to update the `monitoring_services.sh` script at `harvest_status()` function accordingly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Remedy you need a similar template so that the `monitoring_services.sh` can generate an event/ticket\n",
    "> `vi ~/scripts/${USER}/json.SED`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[\n",
    "    {\n",
    "        \"attributes\": {\n",
    "            \"CLASS\": \"STORM_Netapp\",\n",
    "            \"source\": \"NetApp\",\n",
    "            \"severity\": \"MINOR\",\n",
    "            \"origin\": \"monitoring\",\n",
    "            \"sub_origin\": \"TrueSight_REST\",\n",
    "            \"adapter_host\": \"ADAPTER_HOST\",\n",
    "            \"msg\": \"MSG\",\n",
    "            \"contract_id\": \"CONTRACT_ID\",\n",
    "            \"ars_esc\": \"Yes\",\n",
    "            \"ars_delay_time\": \"0\",\n",
    "            \"hostname\": \"HNAME\",\n",
    "            \"mc_host\": \"HNAME\",\n",
    "            \"sub_source\": \"monitoring script\",\n",
    "            \"bmw_esvr_type\": \"BEM\",\n",
    "            \"server_loc\": \"ITA-Lab\",\n",
    "            \"event_id\":\"EVENT_ID\",\n",
    "            \"dd1\":\"critical:HNAME`: SERVICE\",\n",
    "            \"dd2\":\"DATE\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "```\n",
    "Note that the `ADAPTER_HOST`; `MSG`; `CONTRACT_DI`; `HNAME`; `EVENT_ID`; `SERVICE`; `DATE` are automatially replaced with correct values by the monitoring script; other fields are static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Maintenance info backend for UI & insert_uptime for reporting:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ required ] ODBC config\n",
    "> vi <mark>/etc/odbc.ini</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a /etc/odbc.ini file with the content below: (replace servername, username and password with proper values!)\n",
    "[DWH]\n",
    "server = servername\n",
    "#driver = unixodbc\n",
    "driver = /opt/microsoft/msodbcsql18/lib64/libmsodbcsql-18.1.so.1.1\n",
    "database = Common_View\n",
    "username = \"XXXX\"\n",
    "password = 'XXXX'\n",
    "TrustServerCertificate = yes\n",
    "Trace = Yes\n",
    "TraceFile = /home/${USER}/UI/flask_wapi/odbc.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ *optional* ] Details regarding odbc and sql scripts \n",
    "`-- Double click for details --`\n",
    "\n",
    "<!---\n",
    "\n",
    "###### - Bash command to connect to mysql database:\n",
    "> (server, username and password is omitted by \"XXX\" here - to be replaced with real values!)\n",
    "\n",
    "\n",
    "$`cat query_maint`\n",
    "> #!/bin/bash\n",
    "\n",
    "> isql -k \"DRIVER={ODBC Driver 18 for SQL Server};SERVER=XXX,1433;UID=XXX;PWD=XXX;Authentication=SqlPassword;TrustServerCertificate=Yes\" -v -b -d, -q < /home/$[USER}/scripts/maint.sql\n",
    "\n",
    "[isql man](https://www.mankier.com/1/isql#Options)\n",
    "\n",
    "`-b`: Run isql in non-interactive batch mode. In this mode, the isql processes its standard input, expecting one SQL command per line.\n",
    "\n",
    "-dDELIMITER: Delimits columns with delimiter.\n",
    "\n",
    "`-c`: Output the names of the columns on the first row. Has any effect only with the -d or -x options.\n",
    "\n",
    "`-q`: Wrap the character fields in double quotes.\n",
    "\n",
    "$`cat maint.sql`\n",
    "> SELECT * FROM Common_View.monitoring.V_Infrastructure_Maintenance\n",
    "\n",
    "$`query_maint`  # output:\n",
    "> 1,\"Component_Grafana\",\"Grafana\",2022-08-17 00:00:00.0000000,2022-08-18 00:00:00.0000000,\"12324\",\"test chl\"\n",
    "\n",
    "\n",
    "\n",
    "###### - StorageClass with python and pandas:\n",
    "```\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import pymysql\n",
    "from io import StringIO\n",
    "\n",
    "status, storage_df = subprocess.getstatusoutput('query_storage')\n",
    "TESTDATA = StringIO(storage_df)\n",
    "\n",
    "df = pd.read_csv(TESTDATA, sep=\",\")\n",
    "df.shape\n",
    "df.to_csv('/home/$[USER}/storage.csv', index=False)\n",
    "```\n",
    "##### `query_storage`:\n",
    "```\n",
    "similarly as `query_maint` above calls an isql bash command with corresponding SQL (storage.sql):\n",
    "```\n",
    "\n",
    "> SELECT * FROM Common_View.monitoring.V_StorageClass\n",
    "\n",
    "\n",
    "###### - How to export data (to export in html format add \"-w\" after isql (in 'query' script file)\n",
    "isql.sh < commands.sql  # >/dev/null 2>&1\n",
    "\n",
    "\n",
    "\n",
    "###### - Proxy for pacakge installation\n",
    "> pip install <package>  --proxy \"http://USER:localhost@XXXX\"  #HTTP PROXY\n",
    "\n",
    "> pip install <package>  --proxy \"http://USER:localhost@XXXX\"  #HTTPS PROXY\n",
    "\n",
    "> pip install <package>  --proxy \".xxxgroup.net\" #NOPROXY\n",
    "\n",
    "#### Requirements (after installing miniconda)\n",
    "```\n",
    "sudo yum install unixODBC-devel\n",
    "sudo yum -y install gcc gcc-c++ kernel-devel\n",
    "sudo yum -y install python-devel libxslt-devel libffi-devel openssl-devel\n",
    "pip install Flask --proxy \"http://USER:localhost@XXXX\"\n",
    "pip install python-dotenv  --proxy \"http://USER:localhost@XXXX\"\n",
    "pip install pandas --proxy \"http://USER:localhost@XXXX\"\n",
    "pip install pyodbc --proxy \"http://USER:localhost@XXXX\"```\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - [ Backend scripts ] *(continued)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>insert_uptime.sh</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "#cd $flask_path\n",
    "cd /home/${USER}/UI/flask_wapi\n",
    "\n",
    "echo \"\" | tee ./*downtime.rep > /dev/null\n",
    "\n",
    "downtime_check=$(cat *_uptime.txt |  awk '{print $2}' | grep [0-9] |sort -n | head -1)\n",
    "if [[ \" $downtime_check\" -lt  \"1440\" ]]  # if outage happened within the past 24 hours\n",
    " then\tfor service in grafana harvest influx telegraf;\n",
    "\t\tdo start_time=$(cat ${service}_downtime.rep); end_time=$(cat ${service}_*uptime.txt|cut -d'|' -f3 | sort -n | tail -1); date_check=$(echo $end_time | grep [0-9])\n",
    "\t\t\t#if [[ -z \"$date_check\" ]]; then end_time=$(echo `date +\"%Y-%m-%d\"` 23:59:59); fi\n",
    "\t\t\tif [[ -z \"$date_check\" ]]; then end_time=$(echo `date +%Y-%m-%d -d \"yesterday\"` 23:59:59); fi  # as this script is scheduled (cron) to run after midnight\n",
    "\n",
    "\t\t\tif [[ $service = \"grafana\" ]]; then pfrx=\"graf\" \n",
    "\t\t\telif [[ $service = \"harvest\" ]]; then prfx=\"hv\"\n",
    "\t\t\telif [[ $service = \"influx\" ]]; then prfx=\"infl\"\n",
    "\t\t\telif [[ $service = \"harvest\" ]]; then prfx=\"tg\"\n",
    "\t\t\tfi\n",
    "\n",
    "\t\t\tsql=\"INSERT INTO [DATABASE].[dbo].[TABLE]([DWH_Key],[DWH_CreatedBy],[DWH_CreatedDate],[ServerName],[InterruptionStart_UTC],[InterruptionEnd_UTC],[TicketNo]) VALUES ('`hostname`|$service|$start_time', 'insert_uptime.sh', {ts'`date +\"%Y-%m-%d %H:%M:%S\"`'}, '${prfx}_HOSTNAME', {ts'$start_time'}, {ts'$end_time'}, null)\"\n",
    "\n",
    "\t\t\techo \"$sql\" > uptime.sql\n",
    "\t\t\tquery < uptime.sql\n",
    "\t\t\techo Donwtime report sent to database at `date +\"%Y-%m-%d %H:%M:%S\"` >> database_downtime_insertions.log\n",
    "\t\tdone\n",
    " else\n",
    "\techo No downtime in the past 24 hours.\n",
    "fi\n",
    "\n",
    "# note that this requires to have a sulition set up from MSSQL side as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query_maint \n",
    "(note that you have to replace the `server_name`; `user_id`; `password` values below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "isql -k \"DRIVER={ODBC Driver 18 for SQL Server};SERVER=server_name,1433;UID=user_id;PWD=password;Authentication=SqlPassword;TrustServerCertificate=Yes\" -v -b -d, -q < /home/${USER}/scripts/sql/maint.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>maintenance.py</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/${USER}/miniconda3/bin/python\n",
    "\n",
    "import pandas as pd \n",
    "import json \n",
    "import requests \n",
    "import pyodbc \n",
    "import os\n",
    "import subprocess\n",
    "from io import StringIO\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "status, maint_df = subprocess.getstatusoutput('query_maint')\n",
    "DATA = StringIO(maint_df)\n",
    "df = pd.read_csv(DATA, sep=\",\")\n",
    "df.to_csv('/home/${USER}/UI/flask_wapi/maintenance.csv', index=False)\n",
    "#df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>past_incidents.sh</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# TIMER -start\n",
    "res1=$(date +%s.%N)\n",
    "# measure runtime of this script\n",
    "\n",
    "flask_path=\"/home/${USER}/UI/flask_wapi\"\n",
    "cd $flask_path\n",
    "\n",
    "\n",
    "past_incidents()\n",
    "\n",
    "{\n",
    "\n",
    "    for T in MONTH WEEKS DAYS;\n",
    "        do declare t=${T,,};\n",
    "            RANGE=$(date -d \"$date -1 ${t}\" +\"%s\");\n",
    "\n",
    "            rm logfiles 2>/dev/null; for file in *.log; do echo $file >> logfiles; cat logfiles|sort -u > logfiles.list; done\n",
    "            for service in grafana harvest influx telegraf; \n",
    "\t\tdo file=$(cat logfiles.list|grep $service); cat $file|egrep -i '(not|high)' | while read LINE;\n",
    "\t\t    do x=$(echo $LINE |cut -d'@' -f2 | cut -d' ' -f1); \n",
    "\t\t\tif ! [[ $x == '' ]]; then y=$(date -d \"$x\" +\"%s\"); \n",
    "\t\t\t\tif [ \"$RANGE\" -le \"$y\" ]; then echo $LINE >> incidents_${t}.csv; \n",
    "\t\t\t\tfi; \n",
    "\t\t\tfi; \n",
    "\t\t    done; \n",
    "\t\tdone\n",
    "        done\n",
    "\n",
    "cat incidents_days.csv | sort -u > today.csv\n",
    "cat incidents_weeks.csv | sort -u > weekly.csv\n",
    "cat incidents_month.csv | sort -u > montly.csv\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "past_incidents\n",
    "\n",
    "\n",
    "# TIMER STOP (calculate runtime):\n",
    "res2=$(date +%s.%N)\n",
    "dt=$(echo \"$res2 - $res1\" | bc)\n",
    "dd=$(echo \"$dt/86400\" | bc)\n",
    "dt2=$(echo \"$dt-86400*$dd\" | bc)\n",
    "dh=$(echo \"$dt2/3600\" | bc)\n",
    "dt3=$(echo \"$dt2-3600*$dh\" | bc)\n",
    "dm=$(echo \"$dt3/60\" | bc)\n",
    "ds=$(echo \"$dt3-60*$dm\" | bc)\n",
    "echo\n",
    "printf \"script run for: %d:%02d:%02d:%02.4f\\n\" $dd $dh $dm $ds\n",
    "echo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backup\n",
    "> `vi ~/scripts/backup_flask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "dir=\"/home/${USER}/backup\"\n",
    "mkdir -p $dir\n",
    "D=`date +\\%Y\\%m\\%d\\%H\\%M\\%S`; cd $dir\n",
    "\n",
    "config_backup()\n",
    "    {\n",
    "            cp /home/${USER}/.bashrc ${dir}/.bashrc\n",
    "            cp /etc/odbc.ini ${dir}/odbc.ini\n",
    "            cp /etc/logrotate.d/flask ${dir}/flask\n",
    "    }\n",
    "\n",
    "if ! [ -z \"$1\" ]\n",
    " then\n",
    "    config_backup\n",
    "    tar cvzf scripts.tar.gz /home/${USER}/scripts && tar cvzf flask_wapi.tar.gz /home/${USER}/UI/flask_wapi && tar cvzf telegraf.tar.gz /etc/telegraf\n",
    " else\n",
    "    tar cvzf scripts_${D}.tar.gz /home/${USER}/scripts && tar cvzf flask_wapi_${D}.tar.gz /home/${USER}/UI/flask_wapi && tar cvzf telegraf.tar.gz /etc/telegraf\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRONTAB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the crontab via:\n",
    "crontab -e\n",
    "# paste the below:\n",
    "*/1 * * * source /home/${USER}/.bashrc; /home/${USER}/monitoring_services.sh 2>/dev/null  \n",
    "*/1 * * * * source /home/${USER}/bashrc; /home/${USER}/scripts/maintenance.py > /dev/null 2>&1  # runs every minute\n",
    "0 * * * * source /home/${USER}/.bashrc; /home/${USER}/scripts/UI/past_incidents.sh 2>/dev/null  # runs every hour\n",
    "0 20 * * * source /home/${USER}/.bashrc; backup_Flask full > /dev/null 2>&1  # backs up monitoring stuff daily\n",
    "5 0 * * * source /home/${USER}/.bashrc; insert_uptime.sh  # since bashrc is sourced no fullpath is needed\n",
    "@reboot /home/${USER}/UI/f1ask_wapi/run  # starts flask UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logrotate\n",
    "> `vi /etc/logrotate.d/flask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/${USER}/UI/flask_wapi/*uptime.log\n",
    "{\n",
    "    rotate 3\n",
    "    create 0644 ${USER} ${USER}\n",
    "    monthly\n",
    "    size 10M\n",
    "    missingok\n",
    "    dateext\n",
    "    copytruncate\n",
    "    notifempty\n",
    "    compress\n",
    "    delaycompress\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI [ Monitoring Interface ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`http://<telegraf_host_IP>:8000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI Details\n",
    "\n",
    "\n",
    "##### GitLab URL (XXXX internal): \n",
    "`https://XXXX.xxxgroup.net/monitoringsolutions/monitoring_services/-/blob/main/`\n",
    "\n",
    "\n",
    "<b>*routes.py*</b> is the main UI script which renders the html's that are located under `~/UI/flask_wapi/application/templates`\n",
    "\n",
    "<b>*layout.html*</b> is the main html file which contains `css` formatting, `navbar` and `footer` that is present on all pages\n",
    "- <b>*infra.html*</b> is the homepage which shows services status (green if all related jobs are running, yellow if partial, red if dead)\n",
    "> content is dinamically generated via backend script `monitoring_services.sh` and ~/UI/flask_wapi/subprocesses/`*.py`\n",
    "- <b>*past incidents*</b> pill/tab: shows incidents that happened within the past 24 hours / week / month\n",
    "- <b>*maintenance*</b> pill/tab: shows assets that were/are under maintenance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Home Page](https://raw.githubusercontent.com/mmarkus13/flask_monitoring/main/UI/infra.png \"Home Page\")\n",
    "![Past Incidents](https://raw.githubusercontent.com/mmarkus13/flask_monitoring/main/UI/pastincidents.png \"past incidents\")\n",
    "![Maintenance](https://raw.githubusercontent.com/mmarkus13/flask_monitoring/main/UI/maintenance.png \"maintenance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decommission / 'Uninstall' / How to remove the monitoring solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> rm -rf ~/miniconda3  `# also remove related lines from the ~/.bashrc file`\n",
    "\n",
    "> rm -rf ~/scripts ~/UI/flask*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool configurations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - <mark>Telegraf</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info from NetApp:\n",
    "---\n",
    "```\n",
    "I prepared the Grid configuration already, so that the Telegraf can start to collect the data. \n",
    "Please take care that it can take up to 15 minutes with the initial data collection till it get reflected into the InfluxDB with the âprometheusâ _measurements. \n",
    "\n",
    "StorageGRID requires a certificate authentication, so in addition I attached you the required certificates. \n",
    "Move them in the /etc/telegraf directory or subdirectory (modify tls_ca/tls_ca_cert & tls_key path in this case).\n",
    "\n",
    "There are 3 configuration parts to be modified / checked. \n",
    "\n",
    "#1  modify the common Telegraf config (at the beginning of the config file)\n",
    "[agent]\n",
    "   interval = â60sâ\n",
    "   metric_batch_size = 5000\n",
    "   metric_buffer_limit = 75000\n",
    "\n",
    "\n",
    "#2  add the Storagegrid Input config\n",
    " [[inputs.prometheus]] \n",
    "   urls = ['https://XX.X.XX.XX:XXXX/federate?matchXXXX']\n",
    "   metric_version = 2\n",
    "   tls_ca = \"/etc/telegraf/cacert.pem\"\n",
    "   tls_cert = \"/etc/telegraf/cert.pem\"\n",
    "   tls_key = \"/etc/telegraf/key.pem\"\n",
    "   insecure_skip_verify = true\n",
    "   response_timeout = \"59s\"\n",
    "\n",
    "\n",
    "#3 check your [outputs.influxdb_v2]] configuration. \n",
    "Telegraf will write the data into the according bucket you set here. \n",
    "\n",
    "\n",
    "After this restart the Telegraf (via cmd # sudo systemctl stop telegraf & # sudo systemctl start telegraf). \n",
    "15 Minutes after this, the InfluxDB will reflect the StorageGRID data.\n",
    "``` \n",
    "\n",
    "> Source: mail @Fri 22/05/13 13:11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UAT Telegraf config steps (cli)\n",
    "Telegraf configuration\n",
    "Telegraf agents located on: `XXXX` (hostname omitted)\n",
    "\n",
    "> <b>Sources</b>:\n",
    "- SNMP trap receiver\n",
    "- SNMP query for Cisco Switches\n",
    "- VM server data receiver\n",
    "installation folder:\n",
    "`/etc/telegraf`\n",
    "\n",
    "> <b>Services</b>:\n",
    "*each telegraf host has its own sub services*\n",
    ">#### sytemctl status telegraf_broadcom.service\n",
    ">`/usr/bin/telegraf -config /etc/telegraf/telegraf_broadcom.conf -config-directory /etc/telegraf/telegraf_broadcom`\n",
    ">#### sytemctl status telegraf_cisco.service\n",
    ">`/usr/bin/telegraf -config /etc/telegraf/telegraf_cisco.conf -config-directory /etc/telegraf/telegraf_cisco`\n",
    ">#### sytemctl status telegraf_esx.service\n",
    ">`/usr/bin/telegraf -config /etc/telegraf/telegraf_esx.conf -config-directory /etc/telegraf/telegraf_esx`\n",
    ">#### sytemctl status telegraf_storage.service \n",
    ">`/usr/bin/telegraf -config /etc/telegraf/telegraf_storage.conf -config-directory /etc/telegraf/telegraf_storage`\n",
    ">#### sytemctl status telegraf_traps.service\n",
    ">`/usr/bin/telegraf -config /etc/telegraf/telegraf_traps.conf -config-directory /etc/telegraf/telegraf_traps`\n",
    ">#### sytemctl status telegraf_system.service\n",
    ">`/usr/bin/telegraf -config /etc/telegraf/telegraf_system.conf -config-directory /etc/telegraf/telegraf_system`\n",
    "\n",
    "To receive SNMP traps from AIQ UM two MIB file required to copied to the configured path where the MIB's name are important\n",
    "- NETAPP.MIB\n",
    "- OCUM.MIB (this is a renamed aiqum_9.9.mib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - <mark>Influx</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Influx CLI and Modify `bucket's retention`\n",
    "Install Influx CLI/Modify bucket's retention:\n",
    "\n",
    "Download package from the following URL: https://docs.influxdata.com/influxdb/cloud/tools/influx-cli/?t=Windows\n",
    "\n",
    "Install CLI to VDI: Because we haven't permission on the 'C:\\Program Files' folder, need modify the original command:\n",
    "Ori: Expand-Archive .\\influxdb2-client-2.3.0-windows-amd64.zip -DestinationPath 'C:\\Program Files\\InfluxData' \n",
    "\n",
    "mv 'C:\\Program Files\\InfluxData\\influxdb2-client-2.3.0-windows-amd64' 'C:\\Program Files\\InfluxData\\influx'\n",
    "\n",
    "Modified: Expand-Archive .\\influxdb2-client-2.3.0-windows-amd64.zip -DestinationPath 'C:\\InfluxData' mv 'C:\\InfluxData\\influxdb2-client-2.3.0-windows-amd64' 'C:\\InfluxData\\influx'\n",
    "\n",
    "Use Powershell for the following\n",
    "Before issuing the above command, navigate to the folder where you downloaded the CLI package. For example:\n",
    "```\n",
    "cd C:\\Users\"USERNAME\"\\Downloads`\n",
    "mkdir C:\\InfluxData`\n",
    "Expand-Archive .\\influxdb2-client-2.3.0-windows-amd64.zip -DestinationPath 'C:\\InfluxData'\n",
    "mv 'C:\\InfluxData\\influxdb2-client-2.3.0-windows-amd64' 'C:\\InfluxData\\influx'\n",
    "Navigate to the C:\\InfluxData\\influx // because we cannot modify the 'path' variable, need to go to the folder where the influx.exe exists\n",
    "Create an influx CLI's config for the remote host: .\\influx config create -a -n CONFIGNAME -u URL -t TOKEN_WHICH_HAS_PROPER_PRIVILEGES -o ORGANIZATION\n",
    "List bucket's current settings:\n",
    "PS C:\\InfluxData\\influx> .\\influx.exe bucket list ID Name Retention Shard group duration Organization ID Schema Type XXXXX BroadcomBES 1440h0m0s 24h0m0s XXXXX implicit XXXXX CiscoBackend 1440h0m0s 24h0m0s XXXXX implicit\n",
    "Modify bucket's retention: Command reference: https://docs.influxdata.com/influxdb/v2.2/organizations/buckets/update-bucket/\n",
    ".\\influx bucket update -i BUCKET_ID -r NEW_RETENTION_TIME\n",
    "```\n",
    "Done\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
