{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMW Flask API installation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update bashrc for qq user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run once:\n",
    "\n",
    "cleantext=\"\n",
    "export HISTTIMEFORMAT=\"[%Y-%m-%d %H:%M:%S] \"\n",
    "HISTSIZE='INFINITY'; HISTFILESIZE='ANDBEYOND'\n",
    "\n",
    "PS1='\\e[37m\\D{%H:%M}\\e[91m[\\e[90m\\u@\\h \\e[33m\\w\\e[31m]\\e[92m\\n\\$'\n",
    "\n",
    "alias ll='ls -alF'\n",
    "alias la='ls -A'\n",
    "alias l='ls -CmF'\n",
    "alias lr='ls -ltrh'\n",
    "alias ufind=\"find / -name $1 2>/dev/null\"\n",
    "\n",
    "export PATH=$PATH:/home/${USER}/scripts\n",
    "\"\n",
    "\n",
    "echo \"$cleantext\" >> /home/${USER}/.bashrc.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check OS vesion:\n",
    "cat /etc/os-release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(below commands are for RHEL distro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install prereqs with elevated user:\n",
    "\n",
    "# prod:\n",
    "yum install -y libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver bc\n",
    "\n",
    "# dev:\n",
    "subscription-manager repos --enable=codeready-builder-for-rhel-8-x86_64-rpms # required for x11\n",
    "yum install -y libXcomposite libXcursor libXi libXtst libXrandr alsa-lib mesa-libEGL libXdamage mesa-libGL libXScrnSaver bc xorg-x11-apps xorg-x11-xauth firefox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Within VDI]\n",
    "> download <mark>Anaconda3-2020.02-Linux-x86_64.sh</mark> from https://repo.anaconda.com/archive/ \n",
    "\n",
    "> download & install WinSCP from http://wuss.bmwgroup.net\n",
    "\n",
    "> upload Anaconda3-2020.02-Linux-x86_64.sh to <mark>Telegraf host</mark> to /tmp or /home/qqky020/UI/install_files (but first create the folder for that - see below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Telegraf host]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folders:\n",
    "mkdir -p  /home/${USER}/UI/Flask ~/UI/install_files\n",
    "# give execute rights to the installed:\n",
    "chmod +x Anaconda3-2020.02-Linux-x86_64.sh\n",
    "# Long press Enter; then \"yes\"; then \"yes\" (again)\n",
    "# when completed - reload shell:\n",
    ". ~/.bashrc\n",
    "# install requirements for Flask API (maybe you need to update the path to requirements folder: check \"flask_wapi_UAT\")\n",
    "cd  ~/UI/flask_wapi_UAT/requirements; `for': for file in $(ls) ; do pip install ./${file}; done  \n",
    "    # check if all are required (wheel; tar.gz.. some might be duplicates)\n",
    ". ../.flaskenv\n",
    ". ../.flask run\n",
    "# dev/test instance\n",
    "flask run --host 0.0.0.0  \n",
    "# prod\n",
    "IP=\"$(hostname -I | awk '{print $1}')\"\n",
    "nohup flask run --host $IP &\n",
    "# to close it type \"fg\" and press ctrl+c; #or \"kill %1\" #but be sure that is the only background process that is running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VDI]\n",
    "> open preferred web browser\n",
    "\n",
    "> open the UI: http://<mark>XX.X.XX.XX</mark>:8000 #replace with correct IP address <mark># note that the firewall port 8000 has to be opened!</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Telegraf host]\n",
    "\n",
    "Backend & <b>cronjobs</b>: <mark>monitoring_services.sh</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create backend script:\n",
    "vi monitoring_services.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### paste the below // current version = v1.06; 2022.08.18 (Author: Michal Márkus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "# monitoring_services.sh\n",
    "\n",
    "\n",
    "# TIMER -start\n",
    "res1=$(date +%s.%N)\n",
    "# measure runtime of this script\n",
    "\n",
    "DATE=`date +'%m/%d/%Y %H:%M:%S'`; EPOCHNOW=`date -d \"${DATE}\" +\"%s\"`\n",
    "err_msg=\"not running @$DATE\"\n",
    "ok_msg=\"OK @$DATE\"\n",
    "\n",
    "flask_path=/home/qqky020/UI/flask_wapi_UAT\n",
    "cd $flask_path\n",
    "\n",
    "\n",
    "\n",
    "# Define function to check service status:\n",
    "services_check()\n",
    "{\n",
    "\n",
    "# ACTIVE_IQ\n",
    "    # not defined yet\n",
    "\n",
    "# GRAFANA:\n",
    "    grafana_status()\n",
    "    {\n",
    "        #grafana_url=\"https://grafana.apps.kynprodocp.bmwgroup.net/api/health\"  # PROD (port is 443)\n",
    "        grafana_url=\"http://itahdnasrep.bmwgroup.net:3000/api/health\"  # UAT\n",
    "        grafana_check=\"$(curl -s $grafana_url | grep -oh [[:alpha:]]*ok[[:alpha:]]*)\"  # checks if status is \"ok\"\n",
    "        grafana_latency=$(curl -s -w 'Establish Connection: %{time_connect}s\\nTTFB: %{time_starttransfer}s\\nTotal: %{time_total}s\\n' itahdnasrep.bmwgroup.net:3000/ping/api/health | egrep \"Total: [1-9]\") ;  # checks if latency is above 1 second\n",
    "        # log status:\n",
    "        [ $(eval echo \\$\"${service}_check\") == 'ok' ] && [ -z \"$latency\" ] || echo -e \"\\n$DATE\\n$grafana_check\\n\\n###\" >> ${service}_high_latency.log && echo \"${service} $ok_msg\" >> ${service}_uptime.log || echo \"${service}\" $err_msg >> ${service}_uptime.log\n",
    "        export grafana_latency\n",
    "    }\n",
    "\n",
    "# HARVEST:\n",
    "\n",
    "# QQ USER NEEDS TO BE ADDED TO REMOTE HOST & set up PWLESS SSH (or info needs to be posted to this host via Ansible...)\n",
    "    harvest_status()\n",
    "    {\n",
    "        harvest_check=\"$(ssh -tt michal@itahdnasuathar.bmwgroup.net 'systemctl status harvest')\"  # to be replaced with qq user!\n",
    "        if [ \"$(eval echo \\$${service}_check) | sort -u | grep -v running | wc -l)\" -gt 0 ]; then echo \"${service}\" $err_msg  >> ${service}_uptime.log; else echo \"${service} $ok_msg\";fi\n",
    "    }\n",
    "\n",
    "# INFLUX:\n",
    "    influx_status()\n",
    "    {\n",
    "        #influx_url=\"https://influxdb.apps.kynprodocp.bmwgroup.net/health\"  # PROD (port is 443)\n",
    "        influx_url=\"http://itahdnasrep.bmwgroup.net:8086/health\"  # UAT\n",
    "        influx_check=\"$(curl -s $influx_url | grep status |  grep -oh [[:alpha:]]*pass[[:alpha:]]*)\"  # check if status is \"pass\"\n",
    "        influx_latency=$(curl -s -w 'Establish Connection: %{time_connect}s\\nTTFB: %{time_starttransfer}s\\nTotal: %{time_total}s\\n' itahdnasrep.bmwgroup.net:8086 | egrep \"Total: [1-9]\")\n",
    "        [ $(eval echo \\$\"${service}_check\") == 'pass' ] && [ -z \"$latency\" ] || echo -e \"\\n$DATE\\n${service}_check\\n\\n###\" >> ${service}_high_latency.log && echo \"${service} $ok_msg\" >> ${service}_uptime.log || echo \"${service}\" $err_msg  >> ${service}_uptime.log\n",
    "        export influx_latency\n",
    "    }\n",
    "\n",
    "# NodeRed\n",
    "    # not defined yet\n",
    "\n",
    "# TELEGRAF:\n",
    "    telegraf_status()\n",
    "    {\n",
    "        telegraf_check=\"$(systemctl | grep telegraf | sort -u | grep -v running | wc -l)\"\n",
    "        if ! [ \"$(echo $telegraf_check)\" == 0 ]; then echo \"${service}\" $err_msg >> ${service}_uptime.log; else echo \"${service} $ok_msg\" >> ${service}_uptime.log; fi\n",
    "        export telegraf_check\n",
    "    }\n",
    "\n",
    "# LOOP OVER SERVICES:\n",
    "for service in grafana influx telegraf #active_iq harvest nodered\n",
    "    do\n",
    "        ${service}_status\n",
    "    done\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# send ticket if service is down for 5 consecutive minutes\n",
    "ticket()\n",
    "{\n",
    "\n",
    "# ticket details:\n",
    "if\n",
    "    service=grafana; then\n",
    "    eventID=\"123456...\"\n",
    "    resource=\"itahdnasrep\"  # UAT\n",
    "    #state=$(serviceup5min || echo  \"OK\")\n",
    "    state=$grafana_check\n",
    "    latency=$grafana_latency\n",
    "    severity=\"1\"\n",
    "    header=\"Date, Service, Status, EventID, Resource, Severity\"\n",
    "    message=\"$DATE, $service; $state; $eventID, $resource, $severtiy\"\n",
    "elif\n",
    "    service=influx; then\n",
    "    eventID=\"123456...\"\n",
    "    resource=\"itahdnasrep\"  # UAT\n",
    "    state=$influx_check\n",
    "    latency=$influx_latency\n",
    "    severity=\"1\"\n",
    "    header=\"Date, Service, Status, EventID, Resource, Severity\"\n",
    "    message=\"$DATE, $service; $state; $eventID, $resource, $severtiy\"\n",
    "elif\n",
    "    service=telegraf; then\n",
    "    eventID=\"123456...\"\n",
    "    resource=\"itahdnasrep\"  # UAT\n",
    "    state=$telegraf_check\n",
    "    #telegraf_jobs=\"$telegraf_check\"\n",
    "    severity=\"1\"\n",
    "    header=\"Date, Service, Status, EventID, Resource, Severity\"\n",
    "    message=\"$DATE, $service; $state; $eventID, $resource, $severtiy\"\n",
    "    #...\n",
    "fi\n",
    "\n",
    "\n",
    "    # Send info to NodeRed when service is down:\n",
    "    serviceup5min()\n",
    "        {\n",
    "        c=0\n",
    "        for((i=1;i<=5;++i))\n",
    "            do\n",
    "            stat=$(tac ${service}_uptime.log | sed -n \"${i},1p\")\n",
    "            dat=$(echo $stat| cut -d@ -f2)\n",
    "                case $stat in\n",
    "                OK) return 1 ;;\n",
    "                not)\n",
    "                epoch_dat=`date -d \"${dat}\" +\"%s\"`\n",
    "                    if [ \"$(echo $EPOCHNOW-$epoch_dat|bc)\" -le \"360\"  ] # less or equal to 360 seconds AKA 6 min (5min +1min grace time due to latency)\n",
    "                    then c=$((c+1))\n",
    "                    export c\n",
    "                    # if [ \"$c\" == 1 ]; then echo ${service}_down_since \"$epoch_dat\"; fi\n",
    "                fi ;;\n",
    "                esac\n",
    "            done\n",
    "        }\n",
    "\n",
    "\n",
    "    create_ticket()\n",
    "        {\n",
    "            file=${service}_monitoring_ticket_`date +\\%Y\\%m\\%d\\%H\\%M`.json\n",
    "            if [[ $c -ge 5 ]]; then echo -e \"$header\"\\n\"$message\" > $file; else echo $DATE $service - OK; fi\n",
    "        }\n",
    "\n",
    "    for service in grafana influx telegraf #harvest #ansible nodered\n",
    "        do\n",
    "            serviceup5min && create_ticket\n",
    "        done\n",
    "\n",
    "\n",
    "\n",
    "    send_ticket()\n",
    "        {\n",
    "        echo\n",
    "        # *NEEDS TO BE DISCUSSED*\n",
    "        # POST json TO NODERED\n",
    "        }\n",
    "\n",
    "    #send_ticket\n",
    "\n",
    "}\n",
    "\n",
    "manage_logs()\n",
    "    {\n",
    "        # GENERATE UPTIME LOGS FOR FLASK\n",
    "        service_uptime()  # Grafana & Influx\n",
    "        {\n",
    "            if [[ \"$service\" = \"telegraf\" ]]; then\n",
    "                telegraf_sub_service_upt()\n",
    "                    {\n",
    "                    #set -x\n",
    "                    stat=`systemctl status telegraf_${sub}.service`\n",
    "                    [[ $(echo \"$stat\" | grep \"running\") == *running* ]] && r=\"Running\" || r=\"Down\"\n",
    "                    since=$(echo $stat | grep -Po \".*; \\K(.*)(?= ago)\")\n",
    "                    epoch_since=$(date --date=\"$since\" +\"%s\")\n",
    "                    uptime_seconds=`echo $epoch_since - $EPOCHNOW | bc`\n",
    "                    uptime=$(echo $uptime_seconds/60|bc)  # minutes\n",
    "                    \n",
    "                    echo -e \"$r $uptime minutes\" > telegraf_${sub}_uptime.txt\n",
    "                    }\n",
    "\n",
    "                for sub in broadcom cisco esx storage system traps\n",
    "                do\n",
    "                    telegraf_sub_service_upt\n",
    "                done\n",
    "\n",
    "                else\n",
    "                    last=$(tac ${service}_uptime.log | grep -A1 -m 1 \"not\")  # sample: grafana OK @08/11/2022 17:28:03\n",
    "                    up=$(echo \"$last\" | tail -1)\n",
    "                    epoch_up=`date -d \"$(echo $up | cut -d@ -f2)\" +\"%s\"`\n",
    "                    down=$(echo \"$last\" | head -1)\n",
    "                    epoch_down=`date -d \"$(echo $down | cut -d@ -f2)\" +\"%s\"`\n",
    "                    prev_down=$(tac ${service}_uptime.log | grep -m 2 \"not\" | tail -1)\n",
    "                    epoch_prev_down=`date -d \"$(echo $prev_down | cut -d@ -f2)\" +\"%s\"`\n",
    "                    seconds=`echo \"$epoch_down\"-\"$epoch_prev_down\"|bc`\n",
    "\n",
    "                    if [[ \"$seconds\" -eq 0 ]]\n",
    "                        then echo \"UNKNOWN\" > ${service}_uptime.txt\n",
    "                    else\n",
    "                        downtime_minutes=$(echo $seconds/60|bc)\n",
    "                        service_uptime=`echo $(echo \"$EPOCHNOW\"-\"$epoch_up\"|bc)/60|bc`\n",
    "\n",
    "                        echo -e \"Down: $down\\nUp:$up\\nOutage Time: $last_outage minutes\\nUptime: service_uptime\" > ${service}_up_since.txt\n",
    "                    fi\n",
    "            fi\n",
    "        }\n",
    "\n",
    "\n",
    "       for service in grafana influx telegraf #.....\n",
    "        do\n",
    "          service_uptime\n",
    "        done\n",
    "\n",
    "\n",
    "        past_incidents()\n",
    "        {\n",
    "            for T in DAYS WEEKS MONTH;\n",
    "            do declare t=${T,,};\n",
    "                RANGE=$(date -d \"$date -1 ${t}\" +\"%s\");\n",
    "\n",
    "                ls *uptime.log | xargs cat | grep -v OK | sort -u | while read line;\n",
    "                do\n",
    "                    x=$(echo $line |cut -d@ -f2)\n",
    "                    if ! [[ $x == '' ]]; then\n",
    "                        y=$(date -d \"$x\" +\"%s\")\n",
    "                        if [ \"$RANGE\" -le \"$y\" ]; then  echo $line >> incidents_${t}.csv; fi\n",
    "                    fi\n",
    "                done\n",
    "            done\n",
    "\n",
    "\n",
    "        mv incidents_days.csv today.csv 2>/dev/null\n",
    "        mv incidents_weeks.csv weekly.csv 2>/dev/null\n",
    "        mv incidents_month.csv montly.csv 2>/dev/null\n",
    "        }\n",
    "\n",
    "    # call past incidents subfunction\n",
    "    past_incidents\n",
    "\n",
    "}\n",
    "\n",
    "# 2 szintu maintenance check:\n",
    "#        1# local offlne lekérdezés napi 1x\n",
    "#        2# kikuldés előtt  live ellenőzés\n",
    "#        -- PYTHON SCRIPTTEL FOGOM CSINÁLNI... mert cockpitbol pandassal egyszerűbb lekezelni az adatokat\n",
    "\n",
    "\n",
    "# Run main parts of the script:\n",
    "services_check\n",
    "ticket\n",
    "manage_logs\n",
    "\n",
    "\n",
    "# TIMER STOP (calculate runtime):\n",
    "res2=$(date +%s.%N)\n",
    "dt=$(echo \"$res2 - $res1\" | bc)\n",
    "dd=$(echo \"$dt/86400\" | bc)\n",
    "dt2=$(echo \"$dt-86400*$dd\" | bc)\n",
    "dh=$(echo \"$dt2/3600\" | bc)\n",
    "dt3=$(echo \"$dt2-3600*$dh\" | bc)\n",
    "dm=$(echo \"$dt3/60\" | bc)\n",
    "ds=$(echo \"$dt3-60*$dm\" | bc)\n",
    "echo\n",
    "printf \"script run for: %d:%02d:%02d:%02.4f\\n\" $dd $dh $dm $ds\n",
    "echo\n",
    "\n",
    "exit 0\n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "#  PENDING:\n",
    "\n",
    "#--Sandor qq user harvest status lekérdezéshez\n",
    "#--Send info to NodeRed\n",
    "#--Michael Flesh maintenance  (ha nem csv akkor tud e adni accesst MSSQL)\n",
    "    \n",
    "###########################################################################################################  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the crontab via:\n",
    "crontab -e\n",
    "# paste the following (this script runs every minute):\n",
    "*/1 * * * * source ~/.bashrc; /home/qq_XX/monitoring_services.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2022.08.25: STATUS for P1: `80% #progress`\n",
    "\n",
    "### P1 (pending items):\n",
    "\n",
    "> <b>monitoring_services.sh</b> `[1,5%]`: generate_report() --json part; <b>UI</b>: \n",
    "\n",
    "\n",
    "> Enable passwordless ssh for qq user; or copy required files ('systemctl status harvest' output; maintenance.csv [resource,service,start,end] from NodeRed to telegraf host) `[5%]`\n",
    "\n",
    "--currently working on this:\n",
    "> Maintenance mode `[10%]` ; check Active IQ export; update monitoring_services.sh & UI (*check_service.py subprocesses) for maintenance\n",
    "\n",
    "\n",
    "> `+5% extra` <mark>UPTIME</mark> reporting via `curl` or telegraf plugin... | backend completed @2022.08.22; need to add UI elements\n",
    "\n",
    "#### P1 completed items:\n",
    "> Check/debug: <b>monitoring_services.sh</b> `[1,5%]` @2022.08.09\n",
    "\n",
    "> infra.html ` [2%]` @2022.08.09\n",
    "\n",
    "> 'Past Incidents' tab: add uptime `[5%]` @2022.08.18 # completed in terms of backend; need to add to UI as well...\n",
    "\n",
    "#### P2 plan:\n",
    "- Ansible jobs \n",
    "- Performance metrics/alerts/jobs...\n",
    "- Add http<mark>s</mark> & cert to flask\n",
    "\n",
    "#### P3 plan:\n",
    "- CSS formatting\n",
    "- Check redundant packages at <mark>requirements</mark> & check which modules are being used, so that on prod we don't have to install anaconda (minimalistic approach)\n",
    "- Configure <mark>X11</mark> for jupyter on telegraf host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decommission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rm -rf ~/anaconda3  # also remove anaconda stuff from ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODBC config for maintenance & reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### - Bash command to connect to mysql database:\n",
    "> (server, username and password is omitted here - replace with real values!)\n",
    "\n",
    "\n",
    "$cat query_maint\n",
    "> #!/bin/bash\n",
    "isql -k \"DRIVER={ODBC Driver 18 for SQL Server};SERVER=XXX,1433;UID=XXX;PWD=XXX;Authentication=SqlPassword;TrustServerCertificate=Yes\" -v -b -d, -q < /home/qqky020/scripts/maint.sql\n",
    "\n",
    "$cat maint.sql\n",
    "> SELECT * FROM BMW_Common_View.monitoring.V_Infrastructure_Maintenance\n",
    "\n",
    "$query_maint  # output:\n",
    "> 1,\"Component_Grafana\",\"Grafana\",2022-08-17 00:00:00.0000000,2022-08-18 00:00:00.0000000,\"12324\",\"test chl\"\n",
    "\n",
    "\n",
    "\n",
    "###### - How to export data (to export in html form add \"-w\" after isql (in 'query' script file)\n",
    "query < maint.sql  # >/dev/null 2>&1\n",
    "\n",
    "\n",
    "\n",
    "###### - Proxy for pacakge installation\n",
    "> pip install <package>  --proxy \"http://qqky010:Kyndryl&BMW2022@192.109.190.88:8080\"  #HTTP PROXY\n",
    "\n",
    "> pip install <package>  --proxy \"http://qqky010:Kyndryl&BMW2022@192.109.190.88:8080\"  #HTTPS PROXY\n",
    "\n",
    "> pip install <package>  --proxy \".bmwgroup.net\" #NOPROXY\n",
    "\n",
    "#### Requirements (after installing miniconda)\n",
    "```\n",
    "sudo yum install unixODBC-devel\n",
    "sudo yum -y install gcc gcc-c++ kernel-devel\n",
    "sudo yum -y install python-devel libxslt-devel libffi-devel openssl-devel\n",
    "pip install Flask --proxy \"http://qqky010:Kyndryl&BMW2022@192.109.190.88:8080\"\n",
    "pip install python-dotenv  --proxy \"http://qqky010:Kyndryl&BMW2022@192.109.190.88:8080\"\n",
    "pip install pandas --proxy \"http://qqky010:Kyndryl&BMW2022@192.109.190.88:8080\"\n",
    "pip install pyodbc --proxy \"http://qqky010:Kyndryl&BMW2022@192.109.190.88:8080\"```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tool configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegraf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Info from Florian:\n",
    "---\n",
    "```\n",
    "I prepared the Grid configuration already, so that the Telegraf can start to collect the data. \n",
    "Please take care that it can take up to 15 minutes with the initial data collection till it get reflected into the InfluxDB with the “prometheus” _measurements. \n",
    "\n",
    "StorageGRID requires a certificate authentication, so in addition I attached you the required certificates. \n",
    "Move them in the /etc/telegraf directory or subdirectory (modify tls_ca/tls_ca_cert & tls_key path in this case).\n",
    "\n",
    "There are 3 configuration parts to be modified / checked. \n",
    "\n",
    "#1  modify the common Telegraf config (at the beginning of the config file)\n",
    "[agent]\n",
    "   interval = “60s”\n",
    "   metric_batch_size = 5000\n",
    "   metric_buffer_limit = 75000\n",
    "\n",
    "\n",
    "#2  add the Storagegrid Input config\n",
    " [[inputs.prometheus]] \n",
    "   urls = ['https://10.2.62.68:9091/federate?match%5B%5D=%7Bjob%3D~%22.%2B%22%7D']\n",
    "   metric_version = 2\n",
    "   tls_ca = \"/etc/telegraf/cacert.pem\"\n",
    "   tls_cert = \"/etc/telegraf/cert.pem\"\n",
    "   tls_key = \"/etc/telegraf/key.pem\"\n",
    "   insecure_skip_verify = true\n",
    "   response_timeout = \"59s\"\n",
    "\n",
    "\n",
    "#3 check your [outputs.influxdb_v2]] configuration. \n",
    "Telegraf will write the data into the according bucket you set here. \n",
    "\n",
    "\n",
    "After this restart the Telegraf (via cmd # sudo systemctl stop telegraf & # sudo systemctl start telegraf). \n",
    "15 Minutes after this, the InfluxDB will reflect the StorageGRID data.\n",
    "``` \n",
    "\n",
    "> Source: mail @Fri 22/05/13 13:11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UAT Telegraf config steps\n",
    "Telegraf configuration\n",
    "Telegraf agents located on: <b>ITAHDNASUATTEL</b>\n",
    "There is 5 instance running\n",
    "- SNMP trap receiver\n",
    "- SNMP query for Cisco Switches\n",
    "- VM server data receiver\n",
    "installation folder:\n",
    "`/etc/telegraf`\n",
    "each telegraf has its own service\n",
    "#### sytemctl status telegraf_broadcom.service\n",
    "`/usr/bin/telegraf -config /etc/telegraf/telegraf_broadcom.conf -config-directory /etc/telegraf/telegraf_broadcom`\n",
    "#### sytemctl status telegraf_cisco.service\n",
    "`/usr/bin/telegraf -config /etc/telegraf/telegraf_cisco.conf -config-directory /etc/telegraf/telegraf_cisco`\n",
    "#### sytemctl status telegraf_esx.service\n",
    "`/usr/bin/telegraf -config /etc/telegraf/telegraf_esx.conf -config-directory /etc/telegraf/telegraf_esx`\n",
    "#### sytemctl status telegraf_storage.service \n",
    "`/usr/bin/telegraf -config /etc/telegraf/telegraf_storage.conf -config-directory /etc/telegraf/telegraf_storage`\n",
    "#### sytemctl status telegraf_traps.service\n",
    "`/usr/bin/telegraf -config /etc/telegraf/telegraf_traps.conf -config-directory /etc/telegraf/telegraf_traps`\n",
    "#### sytemctl status telegraf_system.service\n",
    "`/usr/bin/telegraf -config /etc/telegraf/telegraf_system.conf -config-directory /etc/telegraf/telegraf_system`\n",
    "\n",
    "To receive SNMP traps from AIQ UM two MIB file required to copied to the configured path where the MIB's name are important\n",
    "- NETAPP.MIB\n",
    "- OCUM.MIB (this is a renamed aiqum_9.9.mib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Influx CLI and Modify <mark>bucket's retention</mark>\n",
    "Install Influx CLI/Modify bucket's retention:\n",
    "Download package from the following URL: https://docs.influxdata.com/influxdb/cloud/tools/influx-cli/?t=Windows\n",
    "Install CLI to VDI: Because we haven't permission on the 'C:\\Program Files' folder, need modify the original command:\n",
    "Ori: Expand-Archive .\\influxdb2-client-2.3.0-windows-amd64.zip -DestinationPath 'C:\\Program Files\\InfluxData' mv 'C:\\Program Files\\InfluxData\\influxdb2-client-2.3.0-windows-amd64' 'C:\\Program Files\\InfluxData\\influx'\n",
    "Modified: Expand-Archive .\\influxdb2-client-2.3.0-windows-amd64.zip -DestinationPath 'C:\\InfluxData' mv 'C:\\InfluxData\\influxdb2-client-2.3.0-windows-amd64' 'C:\\InfluxData\\influx'\n",
    "Use Powershell for the following\n",
    "Before issuing the above command, navigate to the folder where you downloaded the CLI package. For example:\n",
    "```\n",
    "cd C:\\Users\"USERNAME\"\\Downloads`\n",
    "mkdir C:\\InfluxData`\n",
    "Expand-Archive .\\influxdb2-client-2.3.0-windows-amd64.zip -DestinationPath 'C:\\InfluxData'\n",
    "mv 'C:\\InfluxData\\influxdb2-client-2.3.0-windows-amd64' 'C:\\InfluxData\\influx'\n",
    "Navigate to the C:\\InfluxData\\influx // because we cannot modify the 'path' variable, need to go to the folder where the influx.exe exists\n",
    "Create an influx CLI's config for the remote host: .\\influx config create -a -n CONFIGNAME -u URL -t TOKEN_WHICH_HAS_PROPER_PRIVILEGES -o ORGANIZATION\n",
    "List bucket's current settings:\n",
    "PS C:\\InfluxData\\influx> .\\influx.exe bucket list ID Name Retention Shard group duration Organization ID Schema Type 834ba3f797c35789 BroadcomBES 1440h0m0s 24h0m0s f24c8a8d0e5f36e8 implicit f827bf73e326118b CiscoBackend 1440h0m0s 24h0m0s f24c8a8d0e5f36e8 implicit\n",
    "Modify bucket's retention: Command reference: https://docs.influxdata.com/influxdb/v2.2/organizations/buckets/update-bucket/\n",
    ".\\influx bucket update -i BUCKET_ID -r NEW_RETENTION_TIME\n",
    "```\n",
    "Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
